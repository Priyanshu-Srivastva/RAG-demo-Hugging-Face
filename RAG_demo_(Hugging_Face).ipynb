{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9y71sEBLzm2"
      },
      "source": [
        "# **Retrieval-Augmented Generation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4JOFSdKtrFi"
      },
      "source": [
        "**Install dependencies**\n",
        "\n",
        "All these libraries together set up the environment for your project.\n",
        "\n",
        "FAISS :- fast similarity search.\n",
        "\n",
        "Sentence-Transformers :- convert text into embeddings.\n",
        "\n",
        "Transformers :- provide pre-trained models.\n",
        "\n",
        "Gradio :- user interface to demo your model.\n",
        "\n",
        "tqdm :- progress visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "sMVm8dG-qozu"
      },
      "outputs": [],
      "source": [
        "#install required libraries\n",
        "!pip install -q faiss-cpu sentence-transformers transformers==4.44.2 gradio tqdm\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cERT4JgFt2Db"
      },
      "source": [
        "**Imports & device detection**\n",
        "\n",
        "This section imports all required libraries and sets up the environment. It ensures:-\n",
        "\n",
        "File handling (os, glob, pathlib, json).\n",
        "\n",
        "Data management (numpy, tqdm).\n",
        "\n",
        "Deep learning (PyTorch, SentenceTransformers, HuggingFace).\n",
        "\n",
        "Similarity search (FAISS).\n",
        "\n",
        "Device selection (GPU if available, else CPU).\n",
        "\n",
        "os :- Provides functions to interact with the operating system (like file paths, directories, environment variables).\n",
        "\n",
        "glob :- Helps find all file paths matching a specific pattern (like *.txt).\n",
        "\n",
        "SentenceTransformer :- Loads pre-trained models that convert text into semantic embeddings (vectors).\n",
        "\n",
        "faiss :- Facebook AI Similarity Search library, used for fast similarity search & clustering in embeddings space.\n",
        "\n",
        "pipeline (from Hugging Face Transformers) :- Provides ready-to-use NLP pipelines (e.g., summarization, sentiment analysis, question answering)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zhx-I52tQud",
        "outputId": "09d0b510-feba-4b47-958b-1dc9f69b7969"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from transformers import pipeline\n",
        "\n",
        "# Device info\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = 0 if USE_CUDA else -1\n",
        "print(\"CUDA available:\", USE_CUDA)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YmLZGAkt6Y0"
      },
      "source": [
        "Load / upload dataset\n",
        "\n",
        "This block:-\n",
        "\n",
        "Uploads a zip file (countries.zip).\n",
        "\n",
        "Extracts it in Colab.\n",
        "\n",
        "Reads all .txt files inside.\n",
        "\n",
        "Stores their content in documents for later processing.\n",
        "\n",
        "Prints confirmation with previews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "xRib8FIgtTH2",
        "outputId": "ddf28b39-0043-4663-d0c7-4332de2e5e64"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d8c084e5-a2ae-429e-b883-a2f1022ac769\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d8c084e5-a2ae-429e-b883-a2f1022ac769\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving countries.zip to countries (2).zip\n",
            "Extracted files to: /content/\n",
            "Contents: ['.config', 'China.txt', '.ipynb_checkpoints', 'faiss.index', 'India.txt', 'countries (1).zip', 'countries.zip', 'wiki_dataset.json', 'Japan.txt', 'chunks_meta.json', 'countries (2).zip', 'data', '.gradio', 'Russia.txt', 'USA.txt', 'sample_data']\n",
            "Loaded 5 documents\n",
            " - China.txt : China,[i] officially the People's Republic of China (PRC),[j] is a country in Ea ...\n",
            " - India.txt : India, officially the Republic of India,[j][20] is a country in South Asia. It i ...\n",
            " - Japan.txt : Japan[a] is an island country in East Asia. Located in the Pacific Ocean off the ...\n",
            " - Russia.txt : Russia,[b] or the Russian Federation,[c] is a country spanning Eastern Europe an ...\n",
            " - USA.txt : The United States of America (USA), also known as the United States (U.S.) or Am ...\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()   # Select countries.zip\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"countries.zip\"  # uploaded file name\n",
        "extract_path = \"/content/\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Extracted files to:\", extract_path)\n",
        "print(\"Contents:\", os.listdir(extract_path))\n",
        "\n",
        "documents = []\n",
        "for fname in os.listdir(extract_path):\n",
        "    if fname.endswith(\".txt\"):\n",
        "        with open(os.path.join(extract_path, fname), \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "            documents.append((fname, text))\n",
        "\n",
        "print(f\"Loaded {len(documents)} documents\")\n",
        "for name, text in documents:\n",
        "    print(\" -\", name, \":\", text[:80], \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwgAwOJ3t_BD"
      },
      "source": [
        "Chunking function and building chunk\n",
        "\n",
        "This block:\n",
        "\n",
        "Defines a chunking function to split text into overlapping word-based segments.\n",
        "\n",
        "Builds a list of chunks + metadata from all documents.\n",
        "\n",
        "Prints total chunks and previews for validation.\n",
        "\n",
        "This step is crucial because ML/NLP models can’t handle very large documents directly — chunking makes them processable while overlap preserves context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JNK9o80tY76",
        "outputId": "f454ce12-e425-4e6e-90e0-3cb0a93d8d67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total chunks: 20\n"
          ]
        }
      ],
      "source": [
        "# Simple word-based chunker (configurable)\n",
        "CHUNK_MAX_WORDS = 200\n",
        "CHUNK_OVERLAP = 40\n",
        "\n",
        "def chunk_text(text: str, max_words=CHUNK_MAX_WORDS, overlap=CHUNK_OVERLAP) -> List[str]:\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    n = len(words)\n",
        "    while i < n:\n",
        "        j = min(i + max_words, n)\n",
        "        chunk = \" \".join(words[i:j])\n",
        "        chunks.append(chunk)\n",
        "        if j == n:\n",
        "            break\n",
        "        i = j - overlap\n",
        "    return chunks\n",
        "\n",
        "# Build chunks & metadata\n",
        "chunks = []\n",
        "meta = []  # list of dicts: {doc, chunk_idx, global_idx}\n",
        "gidx = 0\n",
        "for fname, text in documents:\n",
        "    doc_chunks = chunk_text(text)\n",
        "    for idx, c in enumerate(doc_chunks):\n",
        "        chunks.append(c)\n",
        "        meta.append({\"doc\": fname, \"chunk_idx\": idx, \"global_idx\": gidx})\n",
        "        gidx += 1\n",
        "\n",
        "print(f\"Total chunks: {len(chunks)}\")\n",
        "if len(chunks) <= 10:\n",
        "    for i, c in enumerate(chunks):\n",
        "        print(i, \"→\", c[:120].replace(\"\\n\",\" \"), \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B86-0-m4uCjX"
      },
      "source": [
        "Load embedding model and embed chunks\n",
        "\n",
        "This block:\n",
        "\n",
        "Loads a pre-trained embedding model (all-MiniLM-L6-v2).\n",
        "\n",
        "Defines a batch embedding function to avoid memory issues.\n",
        "\n",
        "Converts all text chunks into dense vector embeddings.\n",
        "\n",
        "Prepares data for similarity search and NLP tasks.\n",
        "\n",
        "This is the core transformation step where documents become machine-understandable vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "8766f8a7997a4f3faa56b40233e96805",
            "4a42aca4185c4e909c789c068c167027",
            "a240d9d905294e1f8f7db353e3d33240",
            "a96faa559e564fe8906c0b3daa39d3cc",
            "f064892baf6b4235927ea4f2de42ee07",
            "b63083a40e9d4d499275b8fcc4113061",
            "2f3a8d4b7d584066ae94cbc6e05aa6a2",
            "60b34000a401489c9fb6e4354dd59b8d",
            "f5a5f218eb8844f3a74fb66f741c1f96",
            "b7c53101a3274e828fb37311f63f06b6",
            "4d965399dc134f1cb7b5fdc5c6b4b328"
          ]
        },
        "id": "8_MwYOrLtcha",
        "outputId": "2d6db465-d789-46c8-a1de-6f8b5adfa51d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading embedding model: all-MiniLM-L6-v2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8766f8a7997a4f3faa56b40233e96805",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Embedding:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings shape: (20, 384)\n"
          ]
        }
      ],
      "source": [
        "EMBED_MODEL_NAME = \"all-MiniLM-L6-v2\"  # fast & small\n",
        "\n",
        "print(\"Loading embedding model:\", EMBED_MODEL_NAME)\n",
        "embed_model = SentenceTransformer(EMBED_MODEL_NAME, device='cuda' if USE_CUDA else 'cpu')\n",
        "\n",
        "# Embed in batches to avoid memory spikes\n",
        "def embed_texts(texts, batch_size=64):\n",
        "    embs = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        e = embed_model.encode(batch, convert_to_numpy=True, show_progress_bar=False)\n",
        "        embs.append(e)\n",
        "    return np.vstack(embs).astype(\"float32\")\n",
        "\n",
        "embeddings = embed_texts(chunks)\n",
        "print(\"Embeddings shape:\", embeddings.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R94s_tKyuFxN"
      },
      "source": [
        "Build FAISS index\n",
        "\n",
        "This block:\n",
        "\n",
        "Creates a FAISS index with L2 distance for fast similarity search.\n",
        "\n",
        "Adds all embeddings to the index.\n",
        "\n",
        "Saves both the index and metadata to disk for reuse.\n",
        "\n",
        "This is the retrieval backbone — turning your dataset into a searchable vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZlVqd5Xte4g",
        "outputId": "0e11aff0-eae8-474e-a047-3b92519c5104"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FAISS index: ntotal = 20\n",
            "Saved FAISS index and metadata to disk.\n"
          ]
        }
      ],
      "source": [
        "INDEX_FILE = \"/content/faiss.index\"\n",
        "META_FILE = \"/content/chunks_meta.json\"\n",
        "\n",
        "dim = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(embeddings)\n",
        "print(\"FAISS index: ntotal =\", index.ntotal)\n",
        "\n",
        "# Save index and metadata\n",
        "faiss.write_index(index, INDEX_FILE)\n",
        "with open(META_FILE, \"w\", encoding=\"utf-8\") as fh:\n",
        "    json.dump({\"chunks\": chunks, \"meta\": meta}, fh, ensure_ascii=False, indent=2)\n",
        "print(\"Saved FAISS index and metadata to disk.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8zzI41huI50"
      },
      "source": [
        "Retriever function\n",
        "\n",
        "This function:\n",
        "\n",
        "Converts a query into an embedding.\n",
        "\n",
        "Searches FAISS index for the most similar chunks.\n",
        "\n",
        "Returns both the chunk text and its metadata.\n",
        "\n",
        "This is the retrieval step in a Retrieval-Augmented Generation (RAG) pipeline — it connects user queries to the right knowledge chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Pt3j5XbjthR9"
      },
      "outputs": [],
      "source": [
        "def retrieve(query: str, top_k: int = 4):\n",
        "    qvec = embed_model.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
        "    D, I = index.search(qvec, top_k)\n",
        "    results = []\n",
        "    for idx in I[0]:\n",
        "        if idx < 0:\n",
        "            continue\n",
        "        results.append({\"chunk\": chunks[idx], \"meta\": meta[idx], \"idx\": int(idx)})\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvLLNnKJuMHn"
      },
      "source": [
        "Hugging Face LLM\n",
        "\n",
        "This block:\n",
        "\n",
        "Loads a text generation model (Flan-T5 or similar).\n",
        "\n",
        "Defines generate_answer that:\n",
        "\n",
        "Builds a prompt with retrieved context + question.\n",
        "\n",
        "Calls the generator to produce a natural language answer.\n",
        "\n",
        "This is the final stage of a RAG pipeline — retrieval (FAISS) + generation (Flan-T5).\n",
        "\n",
        "In simple words: this is where your system becomes a QA assistant that answers based on your dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLpqvg5otkHz",
        "outputId": "b58e1648-e88f-4a7b-b42b-1b17d0da49d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading HF generator: google/flan-t5-small\n"
          ]
        }
      ],
      "source": [
        "# Choose a model:\n",
        "# - \"google/flan-t5-small\" -> lightweight, good for Colab free\n",
        "# - \"google/flan-t5-base\"  -> better, still reasonable\n",
        "# - \"mistralai/Mistral-7B-Instruct-v0.2\" -> much better but requires large GPU / RAM\n",
        "\n",
        "HF_MODEL = \"google/flan-t5-small\"   # change here if you want a different HF model\n",
        "\n",
        "print(\"Loading HF generator:\", HF_MODEL)\n",
        "# We use text2text-generation pipeline (Flan-T5 expects text2text)\n",
        "generator = pipeline(\"text2text-generation\", model=HF_MODEL, device=device, max_new_tokens=200, do_sample=False)\n",
        "\n",
        "def generate_answer(query: str, retrieved, max_new_tokens=200):\n",
        "    context = \"\\n\\n\".join([r[\"chunk\"] for r in retrieved])\n",
        "    prompt = f\"Use the context below to answer the question. If the answer is not contained in the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    out = generator(prompt, max_new_tokens=max_new_tokens, do_sample=False)\n",
        "    # pipeline returns list of dicts with 'generated_text'\n",
        "    return out[0][\"generated_text\"].strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpnHyWTSzEsp"
      },
      "source": [
        "Simple Gradio chat UI\n",
        "\n",
        "This is a complete Retrieval-Augmented Generation (RAG) chatbot pipeline:\n",
        "\n",
        "User asks a question in Gradio UI.\n",
        "\n",
        "retrieve finds relevant chunks from your documents.\n",
        "\n",
        "generate_answer uses Flan-T5 (or whichever model) to answer using only retrieved context.\n",
        "\n",
        "UI shows answer + evidence snippets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "vnhlQfoSzJm3",
        "outputId": "5ce1ca9f-5a3e-4b37-e0e3-70edefe72b9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Launching Gradio UI (press the public link or open inline).\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://6e7f402bb7be0a526f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://6e7f402bb7be0a526f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def rag_chat(user_input):\n",
        "    if not user_input.strip():\n",
        "        return \"Please ask a question.\"\n",
        "    retrieved = retrieve(user_input, top_k=4)\n",
        "    answer = generate_answer(user_input, retrieved)\n",
        "    # This include small retrieved snippets in the UI\n",
        "    snippets = \"\\n\\n\".join([f\"[{r['meta']['doc']}#{r['meta']['chunk_idx']}]: {r['chunk'][:200]}...\" for r in retrieved])\n",
        "    return f\"**Answer:**\\n{answer}\\n\\n**Retrieved snippets:**\\n{snippets}\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## RAG demo (Hugging Face) — ask questions about the uploaded docs\")\n",
        "    inp = gr.Textbox(lines=2, placeholder=\"Ask a question...\")\n",
        "    out = gr.Markdown()\n",
        "    btn = gr.Button(\"Ask\")\n",
        "    btn.click(lambda q: rag_chat(q), inputs=inp, outputs=out)\n",
        "    # Allow hitting Enter to submit\n",
        "    inp.submit(lambda q: rag_chat(q), inp, out)\n",
        "\n",
        "print(\"Launching Gradio UI (press the public link or open inline).\")\n",
        "demo.launch(share=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2f3a8d4b7d584066ae94cbc6e05aa6a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a42aca4185c4e909c789c068c167027": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b63083a40e9d4d499275b8fcc4113061",
            "placeholder": "​",
            "style": "IPY_MODEL_2f3a8d4b7d584066ae94cbc6e05aa6a2",
            "value": "Embedding: 100%"
          }
        },
        "4d965399dc134f1cb7b5fdc5c6b4b328": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60b34000a401489c9fb6e4354dd59b8d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8766f8a7997a4f3faa56b40233e96805": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a42aca4185c4e909c789c068c167027",
              "IPY_MODEL_a240d9d905294e1f8f7db353e3d33240",
              "IPY_MODEL_a96faa559e564fe8906c0b3daa39d3cc"
            ],
            "layout": "IPY_MODEL_f064892baf6b4235927ea4f2de42ee07"
          }
        },
        "a240d9d905294e1f8f7db353e3d33240": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60b34000a401489c9fb6e4354dd59b8d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f5a5f218eb8844f3a74fb66f741c1f96",
            "value": 1
          }
        },
        "a96faa559e564fe8906c0b3daa39d3cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7c53101a3274e828fb37311f63f06b6",
            "placeholder": "​",
            "style": "IPY_MODEL_4d965399dc134f1cb7b5fdc5c6b4b328",
            "value": " 1/1 [00:00&lt;00:00,  5.75it/s]"
          }
        },
        "b63083a40e9d4d499275b8fcc4113061": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7c53101a3274e828fb37311f63f06b6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f064892baf6b4235927ea4f2de42ee07": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5a5f218eb8844f3a74fb66f741c1f96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
