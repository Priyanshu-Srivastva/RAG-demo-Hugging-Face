{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Retrieval-Augmented Generation**"
      ],
      "metadata": {
        "id": "ymf8wsOsUp84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install dependencies**\n",
        "\n",
        "All these libraries together set up the environment for your project.\n",
        "\n",
        "FAISS :- fast similarity search.\n",
        "\n",
        "Sentence-Transformers :- convert text into embeddings.\n",
        "\n",
        "Transformers :- provide pre-trained models.\n",
        "\n",
        "Gradio :- user interface to demo your model.\n",
        "\n",
        "tqdm :- progress visualization."
      ],
      "metadata": {
        "id": "LvRhiO6wUw2q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lDB330iUf3Z"
      },
      "outputs": [],
      "source": [
        "#install required libraries\n",
        "!pip install -q faiss-cpu sentence-transformers transformers==4.44.2 gradio tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports & device detection**\n",
        "\n",
        "This section imports all required libraries and sets up the environment. It ensures:-\n",
        "\n",
        "File handling (os, glob, pathlib, json).\n",
        "\n",
        "Data management (numpy, tqdm).\n",
        "\n",
        "Deep learning (PyTorch, SentenceTransformers, HuggingFace).\n",
        "\n",
        "Similarity search (FAISS).\n",
        "\n",
        "Device selection (GPU if available, else CPU).\n",
        "\n",
        "os :- Provides functions to interact with the operating system (like file paths, directories, environment variables).\n",
        "\n",
        "glob :- Helps find all file paths matching a specific pattern (like *.txt).\n",
        "\n",
        "SentenceTransformer :- Loads pre-trained models that convert text into semantic embeddings (vectors).\n",
        "\n",
        "faiss :- Facebook AI Similarity Search library, used for fast similarity search & clustering in embeddings space.\n",
        "\n",
        "pipeline (from Hugging Face Transformers) :- Provides ready-to-use NLP pipelines (e.g., summarization, sentiment analysis, question answering)."
      ],
      "metadata": {
        "id": "lVmnkmZKU6Pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from transformers import pipeline\n",
        "\n",
        "# Device info\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = 0 if USE_CUDA else -1\n",
        "print(\"CUDA available:\", USE_CUDA)\n"
      ],
      "metadata": {
        "id": "0Iqtw9ckU009"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load / upload dataset**\n",
        "\n",
        "This block:-\n",
        "\n",
        "Uploads a zip file (countries.zip).\n",
        "\n",
        "Extracts it in Colab.\n",
        "\n",
        "Reads all .txt files inside.\n",
        "\n",
        "Stores their content in documents for later processing.\n",
        "\n",
        "Prints confirmation with previews."
      ],
      "metadata": {
        "id": "HpwWgMtYVGPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()   # Select countries.zip\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"countries.zip\"  # uploaded file name\n",
        "extract_path = \"/content/\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Extracted files to:\", extract_path)\n",
        "print(\"Contents:\", os.listdir(extract_path))\n",
        "\n",
        "documents = []\n",
        "for fname in os.listdir(extract_path):\n",
        "    if fname.endswith(\".txt\"):\n",
        "        with open(os.path.join(extract_path, fname), \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "            documents.append((fname, text))\n",
        "\n",
        "print(f\"Loaded {len(documents)} documents\")\n",
        "for name, text in documents:\n",
        "    print(\" -\", name, \":\", text[:80], \"...\")\n"
      ],
      "metadata": {
        "id": "H-ajHLtHVNvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chunking function and building chunk**\n",
        "\n",
        "This block:-\n",
        "\n",
        "Defines a chunking function to split text into overlapping word-based segments.\n",
        "\n",
        "Builds a list of chunks + metadata from all documents.\n",
        "\n",
        "Prints total chunks and previews for validation.\n",
        "\n",
        "This step is crucial because ML/NLP models can’t handle very large documents directly — chunking makes them processable while overlap preserves context."
      ],
      "metadata": {
        "id": "GV4lyEVlVRBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple word-based chunker (configurable)\n",
        "CHUNK_MAX_WORDS = 200\n",
        "CHUNK_OVERLAP = 40\n",
        "\n",
        "def chunk_text(text: str, max_words=CHUNK_MAX_WORDS, overlap=CHUNK_OVERLAP) -> List[str]:\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    n = len(words)\n",
        "    while i < n:\n",
        "        j = min(i + max_words, n)\n",
        "        chunk = \" \".join(words[i:j])\n",
        "        chunks.append(chunk)\n",
        "        if j == n:\n",
        "            break\n",
        "        i = j - overlap\n",
        "    return chunks\n",
        "\n",
        "# Build chunks & metadata\n",
        "chunks = []\n",
        "meta = []  # list of dicts: {doc, chunk_idx, global_idx}\n",
        "gidx = 0\n",
        "for fname, text in documents:\n",
        "    doc_chunks = chunk_text(text)\n",
        "    for idx, c in enumerate(doc_chunks):\n",
        "        chunks.append(c)\n",
        "        meta.append({\"doc\": fname, \"chunk_idx\": idx, \"global_idx\": gidx})\n",
        "        gidx += 1\n",
        "\n",
        "print(f\"Total chunks: {len(chunks)}\")\n",
        "if len(chunks) <= 10:\n",
        "    for i, c in enumerate(chunks):\n",
        "        print(i, \"→\", c[:120].replace(\"\\n\",\" \"), \"...\")\n"
      ],
      "metadata": {
        "id": "3WD5om8mVUeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load embedding model and embed chunks**\n",
        "\n",
        "This block:-\n",
        "\n",
        "Loads a pre-trained embedding model (all-MiniLM-L6-v2).\n",
        "\n",
        "Defines a batch embedding function to avoid memory issues.\n",
        "\n",
        "Converts all text chunks into dense vector embeddings.\n",
        "\n",
        "Prepares data for similarity search and NLP tasks.\n",
        "\n",
        "This is the core transformation step where documents become machine-understandable vectors."
      ],
      "metadata": {
        "id": "r_4BeJWuVXBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBED_MODEL_NAME = \"all-MiniLM-L6-v2\"  # fast & small\n",
        "\n",
        "print(\"Loading embedding model:\", EMBED_MODEL_NAME)\n",
        "embed_model = SentenceTransformer(EMBED_MODEL_NAME, device='cuda' if USE_CUDA else 'cpu')\n",
        "\n",
        "# Embed in batches to avoid memory spikes\n",
        "def embed_texts(texts, batch_size=64):\n",
        "    embs = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        e = embed_model.encode(batch, convert_to_numpy=True, show_progress_bar=False)\n",
        "        embs.append(e)\n",
        "    return np.vstack(embs).astype(\"float32\")\n",
        "\n",
        "embeddings = embed_texts(chunks)\n",
        "print(\"Embeddings shape:\", embeddings.shape)\n"
      ],
      "metadata": {
        "id": "TW94jZeDViGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build FAISS index**\n",
        "\n",
        "This block:-\n",
        "\n",
        "Creates a FAISS index with L2 distance for fast similarity search.\n",
        "\n",
        "Adds all embeddings to the index.\n",
        "\n",
        "Saves both the index and metadata to disk for reuse.\n",
        "\n",
        "This is the retrieval backbone — turning your dataset into a searchable vector database."
      ],
      "metadata": {
        "id": "Z8_W_nimVkm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INDEX_FILE = \"/content/faiss.index\"\n",
        "META_FILE = \"/content/chunks_meta.json\"\n",
        "\n",
        "dim = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(embeddings)\n",
        "print(\"FAISS index: ntotal =\", index.ntotal)\n",
        "\n",
        "# Save index and metadata\n",
        "faiss.write_index(index, INDEX_FILE)\n",
        "with open(META_FILE, \"w\", encoding=\"utf-8\") as fh:\n",
        "    json.dump({\"chunks\": chunks, \"meta\": meta}, fh, ensure_ascii=False, indent=2)\n",
        "print(\"Saved FAISS index and metadata to disk.\")\n"
      ],
      "metadata": {
        "id": "UCzPqb6FVraS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Retriever function**\n",
        "\n",
        "This function:-\n",
        "\n",
        "Converts a query into an embedding.\n",
        "\n",
        "Searches FAISS index for the most similar chunks.\n",
        "\n",
        "Returns both the chunk text and its metadata.\n",
        "\n",
        "This is the retrieval step in a Retrieval-Augmented Generation (RAG) pipeline — it connects user queries to the right knowledge chunks."
      ],
      "metadata": {
        "id": "iQtXiv68Vutg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(query: str, top_k: int = 4):\n",
        "    qvec = embed_model.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
        "    D, I = index.search(qvec, top_k)\n",
        "    results = []\n",
        "    for idx in I[0]:\n",
        "        if idx < 0:\n",
        "            continue\n",
        "        results.append({\"chunk\": chunks[idx], \"meta\": meta[idx], \"idx\": int(idx)})\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "hCUnRl1qV1aS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hugging Face LLM**\n",
        "\n",
        "This block:-\n",
        "\n",
        "Loads a text generation model (Flan-T5 or similar).\n",
        "\n",
        "Defines generate_answer that:\n",
        "\n",
        "Builds a prompt with retrieved context + question.\n",
        "\n",
        "Calls the generator to produce a natural language answer.\n",
        "\n",
        "This is the final stage of a RAG pipeline — retrieval (FAISS) + generation (Flan-T5).\n",
        "\n",
        "In simple words: this is where your system becomes a QA assistant that answers based on your dataset."
      ],
      "metadata": {
        "id": "3lyW0R43V4Dp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a model:\n",
        "# - \"google/flan-t5-small\" -> lightweight, good for Colab free\n",
        "# - \"google/flan-t5-base\"  -> better, still reasonable\n",
        "# - \"mistralai/Mistral-7B-Instruct-v0.2\" -> much better but requires large GPU / RAM\n",
        "\n",
        "HF_MODEL = \"google/flan-t5-small\"   # change here if you want a different HF model\n",
        "\n",
        "print(\"Loading HF generator:\", HF_MODEL)\n",
        "# We use text2text-generation pipeline (Flan-T5 expects text2text)\n",
        "generator = pipeline(\"text2text-generation\", model=HF_MODEL, device=device, max_new_tokens=200, do_sample=False)\n",
        "\n",
        "def generate_answer(query: str, retrieved, max_new_tokens=200):\n",
        "    context = \"\\n\\n\".join([r[\"chunk\"] for r in retrieved])\n",
        "    prompt = f\"Use the context below to answer the question. If the answer is not contained in the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    out = generator(prompt, max_new_tokens=max_new_tokens, do_sample=False)\n",
        "    # pipeline returns list of dicts with 'generated_text'\n",
        "    return out[0][\"generated_text\"].strip()\n"
      ],
      "metadata": {
        "id": "4S3IFZapV_Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple Gradio chat UI**\n",
        "\n",
        "This is a complete Retrieval-Augmented Generation (RAG) chatbot pipeline:-\n",
        "\n",
        "User asks a question in Gradio UI.\n",
        "\n",
        "retrieve finds relevant chunks from your documents.\n",
        "\n",
        "generate_answer uses Flan-T5 (or whichever model) to answer using only retrieved context.\n",
        "\n",
        "UI shows answer + evidence snippets."
      ],
      "metadata": {
        "id": "NkPybAlQWDDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def rag_chat(user_input):\n",
        "    if not user_input.strip():\n",
        "        return \"Please ask a question.\"\n",
        "    retrieved = retrieve(user_input, top_k=4)\n",
        "    answer = generate_answer(user_input, retrieved)\n",
        "    # This include small retrieved snippets in the UI\n",
        "    snippets = \"\\n\\n\".join([f\"[{r['meta']['doc']}#{r['meta']['chunk_idx']}]: {r['chunk'][:200]}...\" for r in retrieved])\n",
        "    return f\"**Answer:**\\n{answer}\\n\\n**Retrieved snippets:**\\n{snippets}\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## RAG demo (Hugging Face) — ask questions about the uploaded docs\")\n",
        "    inp = gr.Textbox(lines=2, placeholder=\"Ask a question...\")\n",
        "    out = gr.Markdown()\n",
        "    btn = gr.Button(\"Ask\")\n",
        "    btn.click(lambda q: rag_chat(q), inputs=inp, outputs=out)\n",
        "    # Allow hitting Enter to submit\n",
        "    inp.submit(lambda q: rag_chat(q), inp, out)\n",
        "\n",
        "print(\"Launching Gradio UI (press the public link or open inline).\")\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "id": "hlwC2zDhWIDu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
